{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "415a54b9",
   "metadata": {},
   "source": [
    "# Neuroglancer Builder — General (OME‑TIFF, TIFF, or BIGTIFF 2D histology data → 4 split layers)\n",
    "\n",
    "This notebook builds four single‑channel **Neuroglancer precomputed** layers (**DAPI**, **aGFP**, **aRFP**, **aTH**) from your QuPath‑exported OME‑TIFFs.\n",
    "\n",
    "**Highlights**\n",
    "- Finds and **sorts sections** so that `s1-s2` comes **before** `s2-s3`, then by plate (P1..), row (A/B/C), column (1..4).\n",
    "- Creates **one layer per channel**, on your **D:\\\\** drive (Windows path safe `file://D:/...`).\n",
    "- Writes all tiles, **padding with zeros** outside each section’s bounds (no more 404s).\n",
    "- Optional **pre-seed** step writes zeros for *every* tile first, so even if a tile’s data is entirely outside bounds, a file still exists.\n",
    "- Includes a quick **post-check** for missing tiles, and a ready-to-use local server + Neuroglancer URL snippet.\n",
    "\n",
    "> Run top-to-bottom after editing the **Config** cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b329c2e9",
   "metadata": {},
   "source": [
    "# Uncompress exported QuPath TIF\n",
    "\n",
    "*** Adjust these SRC/DST paths only, then copy and paste entire script into command prompt ***\n",
    "\n",
    "```bat\n",
    "set \"SRC=F:\\QuPath_cjSwan-Endogenous\\exports_fullres_ome_clean\"\n",
    "set \"DST=F:\\QuPath_cjSwan-Endogenous\\exports_fullres_ome_uncompressed\"\n",
    "\n",
    "mkdir \"%DST%\" 2>nul\n",
    "\n",
    "echo === Rewriting OME-TIFFs as series 0, Uncompressed, BigTIFF, tiled 512x512 ===\n",
    "for %F in (\"%SRC%\\*.ome.tif\" \"%SRC%\\*.ome.tiff\") do (\n",
    "  echo.\n",
    "  echo -> %~nxF\n",
    "  \"C:\\bftools\\bfconvert.bat\" -no-upgrade -overwrite ^\n",
    "    -series 0 ^\n",
    "    -bigtiff ^\n",
    "    -tilex 512 -tiley 512 ^\n",
    "    -compression Uncompressed ^\n",
    "    \"%~fF\" \"%DST%\\%~nxF\"\n",
    ")\n",
    "\n",
    "echo.\n",
    "echo === Verifying with showinf (nonzero exit => BAD) ===\n",
    "for %F in (\"%DST%\\*.ome.tif\" \"%DST%\\*.ome.tiff\") do @(\"C:\\bftools\\showinf.bat\" -no-upgrade -nopix \"%~fF\" >nul 2>&1) || echo BAD: %~nxF\n",
    "\n",
    "echo.\n",
    "echo Done. Clean files in:\n",
    "echo   %DST%\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f388fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]\n",
      "NumPy: 2.0.2\n",
      "tifffile: 2024.8.30\n",
      "imagecodecs: 2021.8.26\n",
      "zarr: 2.18.2\n",
      "numcodecs: 0.12.1\n",
      "tqdm: 4.62.3\n"
     ]
    }
   ],
   "source": [
    "# Imports & versions\n",
    "import sys, os, re, json, math, numpy as np, tifffile as tiff, zarr\n",
    "from pathlib import Path\n",
    "from cloudvolume import CloudVolume\n",
    "\n",
    "print('Python:', sys.version)\n",
    "print('NumPy:', np.__version__)\n",
    "print('tifffile:', tiff.__version__)\n",
    "import imagecodecs, numcodecs\n",
    "print('imagecodecs:', getattr(imagecodecs, '__version__', 'n/a'))\n",
    "print('zarr:', zarr.__version__)\n",
    "print('numcodecs:', numcodecs.__version__)\n",
    "try:\n",
    "    import tqdm; print('tqdm:', tqdm.__version__)\n",
    "except Exception:\n",
    "    print('tqdm: not installed (optional)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5ddba5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_DIR : D:\\BCH-cjDAE8-mHGHpA\\cjSwan\\Confocal\\Endogenous\n",
      "OUTPUT_DIR: D:\\BCH-cjDAE8-mHGHpA\\cjSwan\\cjSwan-confocal-endogenous_neuroglancer_dataset_per_section\n",
      "Layers    : ['DAPI', 'EGFP', 'dTom', 'aTH']\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# Config (EDIT THESE) |\n",
    "# =====================\n",
    "INPUT_DIR  = r'D:\\BCH-cjDAE8-mHGHpA\\cjSwan\\Confocal\\Endogenous'\n",
    "OUTPUT_DIR = r'D:\\BCH-cjDAE8-mHGHpA\\cjSwan\\cjSwan-confocal-endogenous_neuroglancer_dataset_per_section'\n",
    "\n",
    "# Channel & tiling\n",
    "LAYER_NAMES = ['DAPI', 'EGFP', 'dTom', 'aTH'] \n",
    "# LAYER_NAMES = ['DAPI', 'aTH', 'dTom', 'aRFP']     # Channels 0..3 in your OME-TIFF\n",
    "# LAYER_NAMES = [\"DAPI\",\"aGFP\",\"aRFP\",\"aTH\"]\n",
    "TILE_YX     = 2048                                # XY tile edge length\n",
    "CHUNK_Z     = 1\n",
    "ENCODING    = 'raw'                               # or 'gzip' (slower, smaller)\n",
    "DTYPE       = 'uint16'\n",
    "\n",
    "# Resolution in nanometers (XY pixel size; Z spacing between sections)\n",
    "RES_XY_NM   = 1000\n",
    "RES_Z_NM    = 100000\n",
    "\n",
    "# Pre-seed zeros for every tile? (Recommended: True)\n",
    "PRESEED_ALL_TILES = True\n",
    "\n",
    "# =====================\n",
    "# No edits below here  |\n",
    "# =====================\n",
    "OUTPUT_ROOT = Path(OUTPUT_DIR) / 'layers_split'\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('INPUT_DIR :', INPUT_DIR)\n",
    "print('OUTPUT_DIR:', OUTPUT_DIR)\n",
    "print('Layers    :', LAYER_NAMES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c567fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import tifffile as tiff\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "def discover_files(d):\n",
    "    \"\"\"Legacy helper: return only OME-TIFFs\"\"\"\n",
    "    p = Path(d)\n",
    "    files = [str(x) for x in p.glob('*.ome.tif')]\n",
    "    files += [str(x) for x in p.glob('*.ome.tiff')]\n",
    "    return files\n",
    "\n",
    "# Extended regex: supports plate-row-col format AND fallback for channel-index BigTIFFs\n",
    "_name_pat = re.compile(\n",
    "    r'(?:P(?P<plate>\\d+)-(?P<row>[A-Z])(?P<col>\\d)_s(?P<slo>\\d)-s(?P<shi>\\d))|'\n",
    "    r'(?:_(?P<ch>\\d+)_CH\\.btf)$',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def section_sort_key(path_or_name: str):\n",
    "    \"\"\"Sorting key based on plate/row/col or channel index; fallback to filename.\"\"\"\n",
    "    name = Path(path_or_name).name\n",
    "    m = _name_pat.search(name)\n",
    "    if not m:\n",
    "        return (999, 'Z', 99, 99, name.lower())\n",
    "\n",
    "    gd = m.groupdict()\n",
    "    if gd.get('plate'):\n",
    "        plate = int(gd['plate'])\n",
    "        row   = gd['row'].upper()\n",
    "        col   = int(gd['col'])\n",
    "        slo   = int(gd['slo'])\n",
    "        shi   = int(gd['shi'])\n",
    "        return (slo, shi, plate, row, col, name.lower())\n",
    "    elif gd.get('ch'):\n",
    "        ch = int(gd['ch'])\n",
    "        return (0, 0, 0, 'A', ch, name.lower())\n",
    "    else:\n",
    "        return (999, 'Z', 99, 99, name.lower())\n",
    "\n",
    "def file_cloudpath(p: Path) -> str:\n",
    "    p = Path(p).resolve()\n",
    "    drive = p.drive.rstrip(':')\n",
    "    tail  = p.as_posix().split(':', 1)[1]\n",
    "    return f'file://{drive}:{tail}'\n",
    "\n",
    "def ensure_uint16(arr):\n",
    "    if arr.dtype == np.uint16:\n",
    "        return arr\n",
    "    return arr.astype(np.uint16, copy=False)\n",
    "\n",
    "def list_all_tifs(input_dir: str):\n",
    "    \"\"\"Return unique list of OME, plain, and BigTIFFs in input_dir, sorted by section_sort_key.\"\"\"\n",
    "    p = Path(input_dir)\n",
    "    seen = {}\n",
    "    for pat in (\"*.ome.tif\", \"*.ome.tiff\", \"*.tif\", \"*.tiff\", \"*.btf\", \"*.btiff\"):\n",
    "        for x in p.glob(pat):\n",
    "            seen[str(x)] = True\n",
    "    files = list(seen.keys())\n",
    "    return sorted(files, key=section_sort_key)\n",
    "\n",
    "def xy_from_ome(tf: tiff.TiffFile) -> Tuple[Optional[int], Optional[int]]:\n",
    "    \"\"\"Try to read SizeX/SizeY from OME-XML (returns (X,Y) or (None,None)).\"\"\"\n",
    "    try:\n",
    "        xml = getattr(tf, 'ome_metadata', None)\n",
    "        if not xml:\n",
    "            return None, None\n",
    "        root = ET.fromstring(xml)\n",
    "        if '}' in root.tag:\n",
    "            ns = {'ome': root.tag.split('}')[0].strip('{')}\n",
    "            pixels = root.find('.//ome:Pixels', ns)\n",
    "        else:\n",
    "            pixels = root.find('.//Pixels')\n",
    "        if pixels is None:\n",
    "            return None, None\n",
    "        sx = pixels.get('SizeX')\n",
    "        sy = pixels.get('SizeY')\n",
    "        return (int(sx) if sx else None, int(sy) if sy else None)\n",
    "    except Exception:\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7dd95f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 9 TIFF(s) (OME, plain, BigTIFF)\n",
      "Target volume: X=4063, Y=3050, Z=9\n"
     ]
    }
   ],
   "source": [
    "# --- discover files (OME + plain + BigTIFF) ---\n",
    "files = list_all_tifs(INPUT_DIR)\n",
    "print(f'Discovered {len(files)} TIFF(s) (OME, plain, BigTIFF)')\n",
    "\n",
    "if not files:\n",
    "    raise SystemExit('No TIFFs found. Check INPUT_DIR.')\n",
    "\n",
    "maxY = maxX = 0\n",
    "bad = []\n",
    "fallback_used = []\n",
    "\n",
    "for f in files:\n",
    "    name = Path(f).name\n",
    "    # optionally skip label images\n",
    "    if ' label' in name.lower():\n",
    "        continue\n",
    "    try:\n",
    "        with tiff.TiffFile(f) as tf:\n",
    "            try:\n",
    "                # Preferred: series[0] (works for many OME, plain, and BigTIFFs)\n",
    "                ser  = tf.series[0]\n",
    "                axes = ser.axes.upper()\n",
    "                if 'Y' in axes and 'X' in axes:\n",
    "                    Y = ser.shape[axes.find('Y')]\n",
    "                    X = ser.shape[axes.find('X')]\n",
    "                else:\n",
    "                    raise RuntimeError(f\"series[0] axes lack X/Y: {axes}\")\n",
    "            except Exception:\n",
    "                # Fallback 1: OME-XML\n",
    "                X, Y = xy_from_ome(tf)\n",
    "                if X is not None and Y is not None:\n",
    "                    fallback_used.append(name)\n",
    "                else:\n",
    "                    # Fallback 2: first page dimensions (works for plain & BigTIFFs)\n",
    "                    try:\n",
    "                        p0 = tf.pages[0]\n",
    "                        X, Y = int(p0.imagewidth), int(p0.imagelength)\n",
    "                        fallback_used.append(name)\n",
    "                    except Exception as e2:\n",
    "                        bad.append((name, f\"no XY via series/OME/page0 ({e2})\"))\n",
    "                        continue  # skip this file for sizing\n",
    "\n",
    "            maxY = max(maxY, int(Y))\n",
    "            maxX = max(maxX, int(X))\n",
    "    except Exception as e:\n",
    "        bad.append((name, f\"open error: {e}\"))\n",
    "\n",
    "if fallback_used:\n",
    "    print(f\"WARN: Used fallback sizing for {len(fallback_used)} file(s), e.g.:\")\n",
    "    for n in fallback_used[:5]:\n",
    "        print(\" -\", n)\n",
    "\n",
    "if bad:\n",
    "    print('WARN: Some files were skipped for sizing:')\n",
    "    for n, msg in bad[:10]:\n",
    "        print(' -', n, '->', msg)\n",
    "\n",
    "Z = len(files)\n",
    "print(f'Target volume: X={maxX}, Y={maxY}, Z={Z}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97af9019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating layer at: file://D:/BCH-cjDAE8-mHGHpA/cjSwan/cjSwan-confocal-endogenous_neuroglancer_dataset_per_section/layers_split/DAPI\n",
      "  info exists: True -> D:\\BCH-cjDAE8-mHGHpA\\cjSwan\\cjSwan-confocal-endogenous_neuroglancer_dataset_per_section\\layers_split\\DAPI\\info\n",
      "Creating layer at: file://D:/BCH-cjDAE8-mHGHpA/cjSwan/cjSwan-confocal-endogenous_neuroglancer_dataset_per_section/layers_split/EGFP\n",
      "  info exists: True -> D:\\BCH-cjDAE8-mHGHpA\\cjSwan\\cjSwan-confocal-endogenous_neuroglancer_dataset_per_section\\layers_split\\EGFP\\info\n",
      "Creating layer at: file://D:/BCH-cjDAE8-mHGHpA/cjSwan/cjSwan-confocal-endogenous_neuroglancer_dataset_per_section/layers_split/dTom\n",
      "  info exists: True -> D:\\BCH-cjDAE8-mHGHpA\\cjSwan\\cjSwan-confocal-endogenous_neuroglancer_dataset_per_section\\layers_split\\dTom\\info\n",
      "Creating layer at: file://D:/BCH-cjDAE8-mHGHpA/cjSwan/cjSwan-confocal-endogenous_neuroglancer_dataset_per_section/layers_split/aTH\n",
      "  info exists: True -> D:\\BCH-cjDAE8-mHGHpA\\cjSwan\\cjSwan-confocal-endogenous_neuroglancer_dataset_per_section\\layers_split\\aTH\\info\n",
      "Layers ready: ['D:\\\\BCH-cjDAE8-mHGHpA\\\\cjSwan\\\\cjSwan-confocal-endogenous_neuroglancer_dataset_per_section\\\\layers_split\\\\DAPI', 'D:\\\\BCH-cjDAE8-mHGHpA\\\\cjSwan\\\\cjSwan-confocal-endogenous_neuroglancer_dataset_per_section\\\\layers_split\\\\EGFP', 'D:\\\\BCH-cjDAE8-mHGHpA\\\\cjSwan\\\\cjSwan-confocal-endogenous_neuroglancer_dataset_per_section\\\\layers_split\\\\dTom', 'D:\\\\BCH-cjDAE8-mHGHpA\\\\cjSwan\\\\cjSwan-confocal-endogenous_neuroglancer_dataset_per_section\\\\layers_split\\\\aTH']\n"
     ]
    }
   ],
   "source": [
    "def make_layer(path: Path, size_xyz, chunk_xy, enc=ENCODING, dtype=DTYPE):\n",
    "    info = CloudVolume.create_new_info(\n",
    "        num_channels=1,\n",
    "        layer_type='image',\n",
    "        data_type=dtype,\n",
    "        encoding=enc,\n",
    "        resolution=[RES_XY_NM, RES_XY_NM, RES_Z_NM],\n",
    "        voxel_offset=[0, 0, 0],\n",
    "        chunk_size=[chunk_xy, chunk_xy, CHUNK_Z],\n",
    "        volume_size=list(size_xyz),\n",
    "    )\n",
    "    cp = file_cloudpath(path)\n",
    "    print('Creating layer at:', cp)\n",
    "    vol = CloudVolume(cp, info=info, progress=False)\n",
    "    vol.commit_info()\n",
    "    vol.commit_provenance()\n",
    "    infopath = path / 'info'\n",
    "    print('  info exists:', infopath.exists(), '->', infopath)\n",
    "    return vol\n",
    "\n",
    "X, Y = maxX, maxY\n",
    "layer_paths = [OUTPUT_ROOT / name for name in LAYER_NAMES]\n",
    "vols = [make_layer(p, (X, Y, len(files)), TILE_YX) for p in layer_paths]\n",
    "vol_by_name = dict(zip(LAYER_NAMES, vols))\n",
    "print('Layers ready:', [str(p) for p in layer_paths])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d1114af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiling grid: 2 × 2 of 2048\n",
      "Pre-seeding zeros across all tiles (all layers)… this can take a while.\n",
      "  z=1/9 row 1/2\n",
      "  z=1/9 row 2/2\n",
      "  z=2/9 row 1/2\n",
      "  z=2/9 row 2/2\n",
      "  z=3/9 row 1/2\n",
      "  z=3/9 row 2/2\n",
      "  z=4/9 row 1/2\n",
      "  z=4/9 row 2/2\n",
      "  z=5/9 row 1/2\n",
      "  z=5/9 row 2/2\n",
      "  z=6/9 row 1/2\n",
      "  z=6/9 row 2/2\n",
      "  z=7/9 row 1/2\n",
      "  z=7/9 row 2/2\n",
      "  z=8/9 row 1/2\n",
      "  z=8/9 row 2/2\n",
      "  z=9/9 row 1/2\n",
      "  z=9/9 row 2/2\n",
      "Pre-seed done.\n"
     ]
    }
   ],
   "source": [
    "# Pre-seed zeros across all tiles for all 4 layers with correct XY ordering\n",
    "import math, numpy as np\n",
    "from pathlib import Path\n",
    "from cloudvolume import CloudVolume\n",
    "\n",
    "ROOT = Path(OUTPUT_DIR) / \"layers_split\"\n",
    "LAYER_NAMES = ['DAPI', 'EGFP', 'dTom', 'aTH'] \n",
    "# LAYER_NAMES = ['DAPI', 'aTH', 'dTom', 'aRFP']\n",
    "# LAYER_NAMES = [\"DAPI\",\"aGFP\",\"aRFP\",\"aTH\"]\n",
    "\n",
    "# Open layers (Windows-safe file://D:/... path)\n",
    "vols = [CloudVolume(\"file://\" + (ROOT/name).as_posix(), progress=False) for name in LAYER_NAMES]\n",
    "\n",
    "Z = len(files)\n",
    "tilesY = math.ceil(maxY / TILE_YX)\n",
    "tilesX = math.ceil(maxX / TILE_YX)\n",
    "\n",
    "print(f\"Tiling grid: {tilesY} × {tilesX} of {TILE_YX}\")\n",
    "print(\"Pre-seeding zeros across all tiles (all layers)… this can take a while.\")\n",
    "\n",
    "for zi in range(Z):\n",
    "    for ty in range(tilesY):\n",
    "        y0, y1 = ty*TILE_YX, min(maxY, (ty+1)*TILE_YX)\n",
    "        for tx in range(tilesX):\n",
    "            x0, x1 = tx*TILE_YX, min(maxX, (tx+1)*TILE_YX)\n",
    "            bx, by = x1 - x0, y1 - y0\n",
    "\n",
    "            # IMPORTANT: pad is (X, Y, Z=1) — NOT (Y, X, 1)\n",
    "            pad = np.zeros((bx, by, 1), dtype=np.uint16)\n",
    "\n",
    "            # Write to each single-channel layer\n",
    "            for vol in vols:\n",
    "                vol[x0:x1, y0:y1, zi:zi+1] = pad\n",
    "\n",
    "        print(f\"  z={zi+1}/{Z} row {ty+1}/{tilesY}\")\n",
    "print(\"Pre-seed done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "254acf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tiles to layers (series[0], channels 0..3)…\n",
      "[Z 1/9] Stitch_cjSwan-LH_P1-A2_S2_DA8-EGFP_DA8-dTom_aTH-647_Cycle.tif\n",
      "  wrote row 1/2\n",
      "  wrote row 2/2\n",
      "[Z 2/9] Stitch_cjSwan-LH_P1-B2_S2_DA8-EGFP_DA8-dTom_aTH-647_Cycle.tif\n",
      "  wrote row 1/2\n",
      "  wrote row 2/2\n",
      "[Z 3/9] Stitch_cjSwan-LH_P1-C2_S2_DA8-EGFP_DA8-dTom_aTH-647_Cycle.tif\n",
      "  wrote row 1/2\n",
      "  wrote row 2/2\n",
      "[Z 4/9] Stitch_cjSwan-LH_P2-A2_S2_DA8-EGFP_DA8-dTom_aTH-647_Cycle.tif\n",
      "  wrote row 1/2\n",
      "  wrote row 2/2\n",
      "[Z 5/9] Stitch_cjSwan-LH_P2-B2_S2_DA8-EGFP_DA8-dTom_aTH-647_Cycle.tif\n",
      "  wrote row 1/2\n",
      "  wrote row 2/2\n",
      "[Z 6/9] Stitch_cjSwan-LH_P2-C2_S2_DA8-EGFP_DA8-dTom_aTH-647_Cycle.tif\n",
      "  wrote row 1/2\n",
      "  wrote row 2/2\n",
      "[Z 7/9] Stitch_cjSwan-LH_P3-A1_S2_DA8-EGFP_DA8-dTom_aTH-647_Cycle.tif\n",
      "  wrote row 1/2\n",
      "  wrote row 2/2\n",
      "[Z 8/9] Stitch_cjSwan-LH_P3-B1_S2_DA8-EGFP_DA8-dTom_aTH-647_Cycle.tif\n",
      "  wrote row 1/2\n",
      "  wrote row 2/2\n",
      "[Z 9/9] Stitch_cjSwan-LH_P3-C1_S2_DA8-EGFP_DA8-dTom_aTH-647_Cycle.tif\n",
      "  wrote row 1/2\n",
      "  wrote row 2/2\n",
      "All tiles written.\n"
     ]
    }
   ],
   "source": [
    "print('Writing tiles to layers (series[0], channels 0..3)…')\n",
    "\n",
    "for zi, f in enumerate(files):\n",
    "    print(f'[Z {zi+1}/{len(files)}] {Path(f).name}')\n",
    "    with tiff.TiffFile(f) as tf:\n",
    "        ser  = tf.series[0]\n",
    "        axes = ser.axes.upper()\n",
    "        zr   = zarr.open(ser.aszarr(), mode='r')\n",
    "        iY, iX, iC = axes.find('Y'), axes.find('X'), axes.find('C')\n",
    "        if iY == -1 or iX == -1:\n",
    "            raise RuntimeError(f'Missing X/Y axes in series[0]: {f}')\n",
    "        Yc, Xc = ser.shape[iY], ser.shape[iX]\n",
    "        hasC = (iC != -1)\n",
    "\n",
    "        for ty in range(tilesY):\n",
    "            y0, y1 = ty*TILE_YX, min(Y, (ty+1)*TILE_YX)\n",
    "            for tx in range(tilesX):\n",
    "                x0, x1 = tx*TILE_YX, min(X, (tx+1)*TILE_YX)\n",
    "                tile = np.zeros((y1-y0, x1-x0, 4), dtype=np.uint16)\n",
    "                if y0 < Yc and x0 < Xc:\n",
    "                    ys, xs = min(y1, Yc), min(x1, Xc)\n",
    "                    sl = [slice(None)] * zr.ndim\n",
    "                    sl[iY] = slice(y0, ys)\n",
    "                    sl[iX] = slice(x0, xs)\n",
    "                    if hasC:\n",
    "                        sl[iC] = slice(0,4)\n",
    "                        src = np.asarray(zr[tuple(sl)])\n",
    "                        src_yxc = np.transpose(src, (iY, iX, iC))\n",
    "                        h, w, c = src_yxc.shape\n",
    "                        tile[:h, :w, :min(4,c)] = ensure_uint16(src_yxc[..., :4])\n",
    "                    else:\n",
    "                        src = np.asarray(zr[tuple(sl)])\n",
    "                        h, w = src.shape\n",
    "                        tile[:h, :w, 0] = ensure_uint16(src)\n",
    "                # Write channels\n",
    "                for ci, lname in enumerate(LAYER_NAMES):\n",
    "                    vol = vol_by_name[lname]\n",
    "                    arr_xy = tile[..., ci].T\n",
    "                    vol[x0:x1, y0:y1, zi:zi+1] = arr_xy[:, :, None]\n",
    "            print(f'  wrote row {ty+1}/{tilesY}')\n",
    "print('All tiles written.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d7316f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: DAPI  MIP0 size=[4063, 3050, 9]  encoding=raw\n",
      "  -> MIP1 target size=[2032, 1525, 9]\n",
      "    MIP1 z-slice 1/9 done\n",
      "    MIP1 z-slice 2/9 done\n",
      "    MIP1 z-slice 3/9 done\n",
      "    MIP1 z-slice 4/9 done\n",
      "    MIP1 z-slice 5/9 done\n",
      "    MIP1 z-slice 6/9 done\n",
      "    MIP1 z-slice 7/9 done\n",
      "    MIP1 z-slice 8/9 done\n",
      "    MIP1 z-slice 9/9 done\n",
      "  MIP1 complete.\n",
      "  -> MIP2 target size=[1016, 763, 9]\n",
      "    MIP2 z-slice 1/9 done\n",
      "    MIP2 z-slice 2/9 done\n",
      "    MIP2 z-slice 3/9 done\n",
      "    MIP2 z-slice 4/9 done\n",
      "    MIP2 z-slice 5/9 done\n",
      "    MIP2 z-slice 6/9 done\n",
      "    MIP2 z-slice 7/9 done\n",
      "    MIP2 z-slice 8/9 done\n",
      "    MIP2 z-slice 9/9 done\n",
      "  MIP2 complete.\n",
      "  -> MIP3 target size=[508, 382, 9]\n",
      "    MIP3 z-slice 1/9 done\n",
      "    MIP3 z-slice 2/9 done\n",
      "    MIP3 z-slice 3/9 done\n",
      "    MIP3 z-slice 4/9 done\n",
      "    MIP3 z-slice 5/9 done\n",
      "    MIP3 z-slice 6/9 done\n",
      "    MIP3 z-slice 7/9 done\n",
      "    MIP3 z-slice 8/9 done\n",
      "    MIP3 z-slice 9/9 done\n",
      "  MIP3 complete.\n",
      "  -> MIP4 target size=[254, 191, 9]\n",
      "    MIP4 z-slice 1/9 done\n",
      "    MIP4 z-slice 2/9 done\n",
      "    MIP4 z-slice 3/9 done\n",
      "    MIP4 z-slice 4/9 done\n",
      "    MIP4 z-slice 5/9 done\n",
      "    MIP4 z-slice 6/9 done\n",
      "    MIP4 z-slice 7/9 done\n",
      "    MIP4 z-slice 8/9 done\n",
      "    MIP4 z-slice 9/9 done\n",
      "  MIP4 complete.\n",
      "Layer: EGFP  MIP0 size=[4063, 3050, 9]  encoding=raw\n",
      "  -> MIP1 target size=[2032, 1525, 9]\n",
      "    MIP1 z-slice 1/9 done\n",
      "    MIP1 z-slice 2/9 done\n",
      "    MIP1 z-slice 3/9 done\n",
      "    MIP1 z-slice 4/9 done\n",
      "    MIP1 z-slice 5/9 done\n",
      "    MIP1 z-slice 6/9 done\n",
      "    MIP1 z-slice 7/9 done\n",
      "    MIP1 z-slice 8/9 done\n",
      "    MIP1 z-slice 9/9 done\n",
      "  MIP1 complete.\n",
      "  -> MIP2 target size=[1016, 763, 9]\n",
      "    MIP2 z-slice 1/9 done\n",
      "    MIP2 z-slice 2/9 done\n",
      "    MIP2 z-slice 3/9 done\n",
      "    MIP2 z-slice 4/9 done\n",
      "    MIP2 z-slice 5/9 done\n",
      "    MIP2 z-slice 6/9 done\n",
      "    MIP2 z-slice 7/9 done\n",
      "    MIP2 z-slice 8/9 done\n",
      "    MIP2 z-slice 9/9 done\n",
      "  MIP2 complete.\n",
      "  -> MIP3 target size=[508, 382, 9]\n",
      "    MIP3 z-slice 1/9 done\n",
      "    MIP3 z-slice 2/9 done\n",
      "    MIP3 z-slice 3/9 done\n",
      "    MIP3 z-slice 4/9 done\n",
      "    MIP3 z-slice 5/9 done\n",
      "    MIP3 z-slice 6/9 done\n",
      "    MIP3 z-slice 7/9 done\n",
      "    MIP3 z-slice 8/9 done\n",
      "    MIP3 z-slice 9/9 done\n",
      "  MIP3 complete.\n",
      "  -> MIP4 target size=[254, 191, 9]\n",
      "    MIP4 z-slice 1/9 done\n",
      "    MIP4 z-slice 2/9 done\n",
      "    MIP4 z-slice 3/9 done\n",
      "    MIP4 z-slice 4/9 done\n",
      "    MIP4 z-slice 5/9 done\n",
      "    MIP4 z-slice 6/9 done\n",
      "    MIP4 z-slice 7/9 done\n",
      "    MIP4 z-slice 8/9 done\n",
      "    MIP4 z-slice 9/9 done\n",
      "  MIP4 complete.\n",
      "Layer: dTom  MIP0 size=[4063, 3050, 9]  encoding=raw\n",
      "  -> MIP1 target size=[2032, 1525, 9]\n",
      "    MIP1 z-slice 1/9 done\n",
      "    MIP1 z-slice 2/9 done\n",
      "    MIP1 z-slice 3/9 done\n",
      "    MIP1 z-slice 4/9 done\n",
      "    MIP1 z-slice 5/9 done\n",
      "    MIP1 z-slice 6/9 done\n",
      "    MIP1 z-slice 7/9 done\n",
      "    MIP1 z-slice 8/9 done\n",
      "    MIP1 z-slice 9/9 done\n",
      "  MIP1 complete.\n",
      "  -> MIP2 target size=[1016, 763, 9]\n",
      "    MIP2 z-slice 1/9 done\n",
      "    MIP2 z-slice 2/9 done\n",
      "    MIP2 z-slice 3/9 done\n",
      "    MIP2 z-slice 4/9 done\n",
      "    MIP2 z-slice 5/9 done\n",
      "    MIP2 z-slice 6/9 done\n",
      "    MIP2 z-slice 7/9 done\n",
      "    MIP2 z-slice 8/9 done\n",
      "    MIP2 z-slice 9/9 done\n",
      "  MIP2 complete.\n",
      "  -> MIP3 target size=[508, 382, 9]\n",
      "    MIP3 z-slice 1/9 done\n",
      "    MIP3 z-slice 2/9 done\n",
      "    MIP3 z-slice 3/9 done\n",
      "    MIP3 z-slice 4/9 done\n",
      "    MIP3 z-slice 5/9 done\n",
      "    MIP3 z-slice 6/9 done\n",
      "    MIP3 z-slice 7/9 done\n",
      "    MIP3 z-slice 8/9 done\n",
      "    MIP3 z-slice 9/9 done\n",
      "  MIP3 complete.\n",
      "  -> MIP4 target size=[254, 191, 9]\n",
      "    MIP4 z-slice 1/9 done\n",
      "    MIP4 z-slice 2/9 done\n",
      "    MIP4 z-slice 3/9 done\n",
      "    MIP4 z-slice 4/9 done\n",
      "    MIP4 z-slice 5/9 done\n",
      "    MIP4 z-slice 6/9 done\n",
      "    MIP4 z-slice 7/9 done\n",
      "    MIP4 z-slice 8/9 done\n",
      "    MIP4 z-slice 9/9 done\n",
      "  MIP4 complete.\n",
      "Layer: aTH  MIP0 size=[4063, 3050, 9]  encoding=raw\n",
      "  -> MIP1 target size=[2032, 1525, 9]\n",
      "    MIP1 z-slice 1/9 done\n",
      "    MIP1 z-slice 2/9 done\n",
      "    MIP1 z-slice 3/9 done\n",
      "    MIP1 z-slice 4/9 done\n",
      "    MIP1 z-slice 5/9 done\n",
      "    MIP1 z-slice 6/9 done\n",
      "    MIP1 z-slice 7/9 done\n",
      "    MIP1 z-slice 8/9 done\n",
      "    MIP1 z-slice 9/9 done\n",
      "  MIP1 complete.\n",
      "  -> MIP2 target size=[1016, 763, 9]\n",
      "    MIP2 z-slice 1/9 done\n",
      "    MIP2 z-slice 2/9 done\n",
      "    MIP2 z-slice 3/9 done\n",
      "    MIP2 z-slice 4/9 done\n",
      "    MIP2 z-slice 5/9 done\n",
      "    MIP2 z-slice 6/9 done\n",
      "    MIP2 z-slice 7/9 done\n",
      "    MIP2 z-slice 8/9 done\n",
      "    MIP2 z-slice 9/9 done\n",
      "  MIP2 complete.\n",
      "  -> MIP3 target size=[508, 382, 9]\n",
      "    MIP3 z-slice 1/9 done\n",
      "    MIP3 z-slice 2/9 done\n",
      "    MIP3 z-slice 3/9 done\n",
      "    MIP3 z-slice 4/9 done\n",
      "    MIP3 z-slice 5/9 done\n",
      "    MIP3 z-slice 6/9 done\n",
      "    MIP3 z-slice 7/9 done\n",
      "    MIP3 z-slice 8/9 done\n",
      "    MIP3 z-slice 9/9 done\n",
      "  MIP3 complete.\n",
      "  -> MIP4 target size=[254, 191, 9]\n",
      "    MIP4 z-slice 1/9 done\n",
      "    MIP4 z-slice 2/9 done\n",
      "    MIP4 z-slice 3/9 done\n",
      "    MIP4 z-slice 4/9 done\n",
      "    MIP4 z-slice 5/9 done\n",
      "    MIP4 z-slice 6/9 done\n",
      "    MIP4 z-slice 7/9 done\n",
      "    MIP4 z-slice 8/9 done\n",
      "    MIP4 z-slice 9/9 done\n",
      "  MIP4 complete.\n",
      "\n",
      "✅ Multiscale complete (MIP1..MIP4, RAW). Hard refresh Neuroglancer and try zooming out.\n"
     ]
    }
   ],
   "source": [
    "# Build MIP1..MIP4 for all layers (RAW tiles, no gzip).\n",
    "# Neuroglancer will automatically switch to higher MIP indices when you zoom out.\n",
    "\n",
    "import json, math, numpy as np\n",
    "from pathlib import Path\n",
    "from cloudvolume import CloudVolume\n",
    "\n",
    "# --- config ---\n",
    "MAX_MIPS   = 4            # build MIP1..MIP4 (increase to 5/6 if still heavy)\n",
    "CHUNK_XY   = 2048         # keep same chunk size across MIPs\n",
    "LAYER_NAMES = ['DAPI', 'EGFP', 'dTom', 'aTH'] \n",
    "# LAYER_NAMES = [\"DAPI\",\"EGFP\",\"dTom\",\"aTH\"]\n",
    "# LAYER_NAMES = ['DAPI', 'aTH', 'dTom', 'aRFP']\n",
    "\n",
    "ROOT = Path(OUTPUT_DIR) / \"layers_split\"\n",
    "\n",
    "def file_cloudpath(p: Path) -> str:\n",
    "    return \"file://\" + str(Path(p).resolve()).replace(\"\\\\\", \"/\")\n",
    "\n",
    "def ceil_div(a,b): return (a + b - 1) // b\n",
    "\n",
    "def blk_to_xy(blk: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Normalize CloudVolume block (X,Y,1) or (X,Y,1,1) to 2D (X,Y).\"\"\"\n",
    "    arr = np.asarray(blk)\n",
    "    if arr.ndim == 2:\n",
    "        return arr\n",
    "    # collapse trailing axes and take the first slice\n",
    "    arr = arr.reshape(arr.shape[0], arr.shape[1], -1)\n",
    "    return arr[..., 0]\n",
    "\n",
    "def block_mean_2x(src_xy: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Downsample 2D (X,Y) by 2 via 2×2 mean, pad edges by extension if odd.\"\"\"\n",
    "    if src_xy.ndim != 2:\n",
    "        raise ValueError(f\"block_mean_2x expected 2D input, got {src_xy.shape}\")\n",
    "    X, Y = src_xy.shape\n",
    "    padX = X & 1; padY = Y & 1\n",
    "    if padX or padY:\n",
    "        src_xy = np.pad(src_xy, ((0,padX),(0,padY)), mode='edge')\n",
    "        X, Y = src_xy.shape\n",
    "    ds = (\n",
    "        src_xy[0:X:2, 0:Y:2].astype(np.uint32)\n",
    "      + src_xy[1:X:2, 0:Y:2].astype(np.uint32)\n",
    "      + src_xy[0:X:2, 1:Y:2].astype(np.uint32)\n",
    "      + src_xy[1:X:2, 1:Y:2].astype(np.uint32)\n",
    "    ) // 4\n",
    "    return ds.astype(np.uint16)\n",
    "\n",
    "def load_info(layer_path: Path) -> dict:\n",
    "    \"\"\"Read info; if missing/empty, bootstrap from CloudVolume mip 0.\"\"\"\n",
    "    info_path = layer_path / \"info\"\n",
    "    if not info_path.exists() or info_path.stat().st_size == 0:\n",
    "        # bootstrap from the existing dataset on disk\n",
    "        vol0 = CloudVolume(file_cloudpath(layer_path), mip=0, progress=False, compress=False)\n",
    "        info = vol0.info.copy()\n",
    "        # Ensure MIP0 encoding is raw\n",
    "        info[\"scales\"][0][\"encoding\"] = \"raw\"\n",
    "        info[\"scales\"][0][\"chunk_sizes\"] = [[int(CHUNK_XY), int(CHUNK_XY), 1]]\n",
    "        info_path.write_text(json.dumps(info, indent=2))\n",
    "        print(f\"[{layer_path.name}] bootstrapped info from dataset.\")\n",
    "        return info\n",
    "    return json.loads(info_path.read_text())\n",
    "\n",
    "def save_info(layer_path: Path, info: dict) -> None:\n",
    "    (layer_path / \"info\").write_text(json.dumps(info, indent=2))\n",
    "\n",
    "def ensure_raw_scales(info: dict, up_to_mip: int) -> dict:\n",
    "    \"\"\"Force encoding='raw' and ensure MIP entries up to up_to_mip exist with proper sizes/keys.\"\"\"\n",
    "    scales = info.get(\"scales\", [])\n",
    "    if not scales:\n",
    "        raise RuntimeError(\"No 'scales' in info.\")\n",
    "\n",
    "    # Force raw + chunk size on existing scales\n",
    "    for sc in scales:\n",
    "        sc[\"encoding\"] = \"raw\"\n",
    "        sc[\"chunk_sizes\"] = [[int(CHUNK_XY), int(CHUNK_XY), 1]]\n",
    "\n",
    "    # Append scales until we have MIP up_to_mip\n",
    "    while len(scales) <= up_to_mip:\n",
    "        prev = scales[-1]\n",
    "        res  = [int(prev[\"resolution\"][0])*2, int(prev[\"resolution\"][1])*2, int(prev[\"resolution\"][2])]\n",
    "        size = [ceil_div(int(prev[\"size\"][0]),2), ceil_div(int(prev[\"size\"][1]),2), int(prev[\"size\"][2])]\n",
    "        scales.append({\n",
    "            \"encoding\": \"raw\",\n",
    "            \"chunk_sizes\": [[int(CHUNK_XY), int(CHUNK_XY), 1]],\n",
    "            \"key\": f\"{res[0]}_{res[1]}_{res[2]}\",\n",
    "            \"resolution\": res,\n",
    "            \"size\": size,\n",
    "            \"voxel_offset\": prev.get(\"voxel_offset\", [0,0,0]),\n",
    "        })\n",
    "\n",
    "    info[\"scales\"] = scales[:up_to_mip+1]\n",
    "    return info\n",
    "\n",
    "def build_mips_for_layer(layer_path: Path):\n",
    "    # 1) make sure info exists & is raw, extend scales to MAX_MIPS\n",
    "    info = ensure_raw_scales(load_info(layer_path), MAX_MIPS)\n",
    "    save_info(layer_path, info)\n",
    "\n",
    "    s0 = info[\"scales\"][0]\n",
    "    X0, Y0, Z = map(int, s0[\"size\"])\n",
    "    print(f\"Layer: {layer_path.name}  MIP0 size={[X0,Y0,Z]}  encoding={s0['encoding']}\")\n",
    "\n",
    "    # 2) build each MIP from the previous MIP\n",
    "    for mip in range(1, MAX_MIPS+1):\n",
    "        Xm, Ym, Zm = map(int, info[\"scales\"][mip][\"size\"])\n",
    "        print(f\"  -> MIP{mip} target size={[Xm, Ym, Zm]}\")\n",
    "\n",
    "        vol_prev = CloudVolume(file_cloudpath(layer_path), mip=mip-1, progress=False, compress=False)\n",
    "        vol_m    = CloudVolume(file_cloudpath(layer_path), mip=mip,   progress=False, compress=False)\n",
    "\n",
    "        tilesX = math.ceil(Xm / CHUNK_XY)\n",
    "        tilesY = math.ceil(Ym / CHUNK_XY)\n",
    "        prev_size = list(map(int, vol_prev.info[\"scales\"][mip-1][\"size\"]))\n",
    "\n",
    "        for z in range(Zm):\n",
    "            for ix in range(tilesX):\n",
    "                x0m = ix * CHUNK_XY\n",
    "                x1m = min(Xm, (ix+1)*CHUNK_XY)\n",
    "                for iy in range(tilesY):\n",
    "                    y0m = iy * CHUNK_XY\n",
    "                    y1m = min(Ym, (iy+1)*CHUNK_XY)\n",
    "\n",
    "                    # read 2x region from previous MIP (clamped)\n",
    "                    X0a, X1a = x0m*2, min(prev_size[0], x1m*2)\n",
    "                    Y0a, Y1a = y0m*2, min(prev_size[1], y1m*2)\n",
    "\n",
    "                    blk = vol_prev[X0a:X1a, Y0a:Y1a, z:z+1]   # (X,Y,1) or (X,Y,1,1)\n",
    "                    if blk.size == 0:\n",
    "                        ds = np.zeros((x1m-x0m, y1m-y0m), np.uint16)\n",
    "                    else:\n",
    "                        src_xy = blk_to_xy(blk)\n",
    "                        ds = block_mean_2x(src_xy)            # (Xa/2, Ya/2)\n",
    "                        # pad/crop to exact tile dims\n",
    "                        needX = (x1m-x0m) - ds.shape[0]\n",
    "                        needY = (y1m-y0m) - ds.shape[1]\n",
    "                        if needX > 0 or needY > 0:\n",
    "                            ds = np.pad(ds, ((0,max(0,needX)), (0,max(0,needY))), mode='edge')\n",
    "                        ds = ds[:(x1m-x0m), : (y1m-y0m)]\n",
    "\n",
    "                    vol_m[x0m:x1m, y0m:y1m, z:z+1] = ds[:, :, None]\n",
    "            print(f\"    MIP{mip} z-slice {z+1}/{Zm} done\")\n",
    "        print(f\"  MIP{mip} complete.\")\n",
    "\n",
    "# --- run all layers ---\n",
    "for lname in LAYER_NAMES:\n",
    "    build_mips_for_layer(ROOT / lname)\n",
    "\n",
    "print(\"\\n✅ Multiscale complete (MIP1..MIP4, RAW). Hard refresh Neuroglancer and try zooming out.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "124ffb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_key: 1000_1000_100000 | chunk: (2048, 2048, 1) | size: (4063, 3050, 9)\n",
      "Expecting: 36 files per layer\n",
      "[DAPI] OK (all chunks present)\n",
      "[EGFP] MISSING 36 files (showing up to 5): ['0-2048_0-2048_0-1', '2048-4063_0-2048_0-1', '0-2048_2048-3050_0-1', '2048-4063_2048-3050_0-1', '0-2048_0-2048_1-2']\n",
      "[dTom] OK (all chunks present)\n",
      "[aTH] OK (all chunks present)\n",
      "\n",
      "Some tiles are missing — re-run tiles cell or inspect specific coordinates.\n"
     ]
    }
   ],
   "source": [
    "def expected_chunk_names(size_x, size_y, size_z, cx, cy, cz):\n",
    "    nx = math.ceil(size_x / cx); ny = math.ceil(size_y / cy)\n",
    "    out = []\n",
    "    for zi in range(size_z):\n",
    "        for iy in range(ny):\n",
    "            for ix in range(nx):\n",
    "                x0 = ix*cx; x1 = min(size_x, (ix+1)*cx)\n",
    "                y0 = iy*cy; y1 = min(size_y, (iy+1)*cy)\n",
    "                out.append(f'{x0}-{x1}_{y0}-{y1}_{zi}-{zi+1}')\n",
    "    return out, nx, ny\n",
    "\n",
    "# info = json.loads((OUTPUT_ROOT / 'Nissl' / 'info').read_text())\n",
    "info = json.loads((OUTPUT_ROOT / 'DAPI' / 'info').read_text())\n",
    "scale = info['scales'][0]\n",
    "scale_key = scale['key']\n",
    "cx, cy, cz = map(int, scale['chunk_sizes'][0])\n",
    "size_x, size_y, size_z = map(int, scale['size'])\n",
    "expected, nx, ny = expected_chunk_names(size_x, size_y, size_z, cx, cy, cz)\n",
    "\n",
    "print('scale_key:', scale_key, '| chunk:', (cx,cy,cz), '| size:', (size_x,size_y,size_z))\n",
    "print('Expecting:', len(expected), 'files per layer')\n",
    "\n",
    "missing_any = False\n",
    "for lname in LAYER_NAMES:\n",
    "    sdir = OUTPUT_ROOT / lname / scale_key\n",
    "    present = set(p.name for p in sdir.glob('*_*_*') if p.is_file())\n",
    "    missing = [n for n in expected if n not in present]\n",
    "    if missing:\n",
    "        missing_any = True\n",
    "        print(f'[{lname}] MISSING {len(missing)} files (showing up to 5):', missing[:5])\n",
    "    else:\n",
    "        print(f'[{lname}] OK (all chunks present)')\n",
    "\n",
    "if not missing_any:\n",
    "    print('\\nAll layers complete. You can serve & view in Neuroglancer.')\n",
    "else:\n",
    "    print('\\nSome tiles are missing — re-run tiles cell or inspect specific coordinates.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3858c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAPI] decompressed 0 tile(s). Leftover .gz: 0\n",
      "[EGFP] 1000_1000_100000: decompressing 36 tile(s)…\n",
      "[EGFP] decompressed 36 tile(s). Leftover .gz: 0\n",
      "[dTom] decompressed 0 tile(s). Leftover .gz: 0\n",
      "[aTH] decompressed 0 tile(s). Leftover .gz: 0\n",
      "\n",
      "Done. If you’re using `http-server`, restart it or run with cache disabled (e.g. `http-server -c-1 --cors`). Hard refresh Neuroglancer.\n"
     ]
    }
   ],
   "source": [
    "# Decompress any .gz tiles (all MIPs, all layers) and force encoding='raw' in info.\n",
    "# Safe to re-run; skips already-raw tiles and only updates what's needed.\n",
    "\n",
    "import json, gzip, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Uses your existing OUTPUT_DIR and LAYER_NAMES; tweak if needed\n",
    "ROOT = Path(OUTPUT_DIR) / \"layers_split\"\n",
    "try:\n",
    "    LAYER_NAMES\n",
    "except NameError:\n",
    "\n",
    "    LAYER_NAMES = ['DAPI', 'EGFP', 'dTom', 'aTH'] \n",
    "#     LAYER_NAMES = ['DAPI', 'aTH', 'dTom', 'aRFP']\n",
    "#     LAYER_NAMES = [\"DAPI\",\"aGFP\",\"aRFP\",\"aTH\"]\n",
    "\n",
    "def gunzip_to(src_gz: Path, dst_raw: Path):\n",
    "    dst_raw.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with gzip.open(src_gz, 'rb') as fi, open(dst_raw, 'wb') as fo:\n",
    "        shutil.copyfileobj(fi, fo)\n",
    "\n",
    "for lname in LAYER_NAMES:\n",
    "    layer = ROOT / lname\n",
    "    info_path = layer / \"info\"\n",
    "    if not info_path.exists():\n",
    "        print(f\"[{lname}] No info file at {info_path} — skipping.\")\n",
    "        continue\n",
    "\n",
    "    info = json.loads(info_path.read_text())\n",
    "    # Force encoding='raw' for every scale\n",
    "    enc_changed = False\n",
    "    for s in info.get(\"scales\", []):\n",
    "        if s.get(\"encoding\", \"raw\") != \"raw\":\n",
    "            s[\"encoding\"] = \"raw\"\n",
    "            enc_changed = True\n",
    "\n",
    "    total_decompressed = 0\n",
    "    # Walk each scale dir and gunzip tiles\n",
    "    for s in info.get(\"scales\", []):\n",
    "        key = s[\"key\"]\n",
    "        sdir = layer / key\n",
    "        if not sdir.exists():\n",
    "            continue\n",
    "        gz_files = list(sdir.rglob(\"*.gz\"))\n",
    "        if gz_files:\n",
    "            print(f\"[{lname}] {key}: decompressing {len(gz_files):,} tile(s)…\")\n",
    "        for i, gz in enumerate(gz_files, 1):\n",
    "            raw = gz.with_suffix(\"\")  # strip .gz\n",
    "            if raw.exists():\n",
    "                # A raw tile is already present; remove the gz to avoid ambiguity\n",
    "                try:\n",
    "                    gz.unlink()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                continue\n",
    "            try:\n",
    "                gunzip_to(gz, raw)\n",
    "                gz.unlink()\n",
    "                total_decompressed += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  WARN: failed to decompress {gz.name}: {e}\")\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"  …{i} done\")\n",
    "\n",
    "    if enc_changed:\n",
    "        info_path.write_text(json.dumps(info, indent=2))\n",
    "        print(f\"[{lname}] info updated -> encoding='raw' for all scales.\")\n",
    "\n",
    "    leftovers = list((layer).rglob(\"*.gz\"))\n",
    "    print(f\"[{lname}] decompressed {total_decompressed} tile(s). Leftover .gz: {len(leftovers)}\")\n",
    "\n",
    "print(\"\\nDone. If you’re using `http-server`, restart it or run with cache disabled (e.g. `http-server -c-1 --cors`). Hard refresh Neuroglancer.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82bdf4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched: D:\\BCH-cjDAE8-mHGHpA\\cjSwan\\cjSwan-confocal-endogenous_neuroglancer_dataset_per_section\\layers_split\\DAPI\\info\n",
      "Patched: D:\\BCH-cjDAE8-mHGHpA\\cjSwan\\cjSwan-confocal-endogenous_neuroglancer_dataset_per_section\\layers_split\\EGFP\\info\n",
      "Patched: D:\\BCH-cjDAE8-mHGHpA\\cjSwan\\cjSwan-confocal-endogenous_neuroglancer_dataset_per_section\\layers_split\\dTom\\info\n",
      "Patched: D:\\BCH-cjDAE8-mHGHpA\\cjSwan\\cjSwan-confocal-endogenous_neuroglancer_dataset_per_section\\layers_split\\aTH\\info\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Point this to your dataset root (where the per-layer folders live)\n",
    "ROOT = Path(OUTPUT_DIR) / \"layers_split\"\n",
    "# LAYER_NAMES = ['DAPI', 'aTH', 'dTom', 'aRFP']\n",
    "LAYER_NAMES = ['DAPI', 'EGFP', 'dTom', 'aTH'] \n",
    "\n",
    "for lname in LAYER_NAMES:\n",
    "    info_path = ROOT / lname / \"info\"\n",
    "    if not info_path.exists():\n",
    "        print(f\"SKIP: {info_path} not found\")\n",
    "        continue\n",
    "\n",
    "    # Load JSON\n",
    "    with open(info_path, \"r\") as f:\n",
    "        info = json.load(f)\n",
    "\n",
    "    # Add/overwrite the background color field\n",
    "    info[\"default_background_color\"] = [0, 0, 0, 0]\n",
    "\n",
    "    # Save back to disk (pretty formatting so it's easy to inspect)\n",
    "    with open(info_path, \"w\") as f:\n",
    "        json.dump(info, f, indent=2)\n",
    "\n",
    "    print(f\"Patched: {info_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef837e98",
   "metadata": {},
   "source": [
    "# **Serve locally & open in Neuroglancer**\n",
    "\n",
    "\n",
    "## Serve folder with desired Neuroglancer dataset using **cmd**:\n",
    "\n",
    "\n",
    "### cjSwan Ab-Amplified VS200 dataset\n",
    "```bat\n",
    "cd /d D:\\BCH-cjDAE8-mHGHpA\\cjSwan\\VS200-10X\\cjSwan-Amplified_neuroglancer_precomputed\\layers_split\n",
    "http-server . -p 8080 --cors\n",
    "```\n",
    "\n",
    "### cjSwan Endogenous VS200 dataset\n",
    "```bat\n",
    "cd /d F:\\cjSwan-Endogenous_neuroglancer_precomputed\\layers_split\n",
    "http-server . -p 8080 --cors\n",
    "```\n",
    "\n",
    "### cjSwan Ab-Amplified confocal dataset\n",
    "```bat\n",
    "cd /d D:\\BCH-cjDAE8-mHGHpA\\cjSwan\\confocal-amplified_neuroglancer_dataset_per_section\\layers_split\n",
    "http-server . -p 8080 --cors\n",
    "```\n",
    "\n",
    "### cjSwan Endogenous confocal dataset\n",
    "\n",
    "```bat\n",
    "cd /d D:\\BCH-cjDAE8-mHGHpA\\cjSwan\\cjSwan-confocal-endogenous_neuroglancer_dataset_per_section\\layers_split\n",
    "http-server . -p 8080 --cors\n",
    "```\n",
    "\n",
    "### cjDove Ab-Amplified confocal dataset\n",
    "```bat\n",
    "cd /d D:\\cjDove-DAE8-Systemic\\cjDove-confocal_neuroglancer_precomputed\\layers_split\n",
    "http-server . -p 8080 --cors\n",
    "```\n",
    "\n",
    "### Mouse Systemic 5E+10 vg dose\n",
    "```\n",
    "cd /d D:\\BCH-cjDAE8-mHGHpA\\Mouse_Systemic\\Systemic-Injection_5E10\\MouseSystemic_5E10_neuroglancer_precomputed\\layers_split\n",
    "http-server . -p 8080 --cors\n",
    "```\n",
    "\n",
    "### Mouse Systemic 5E+11 vg dose\n",
    "```\n",
    "cd /d D:\\BCH-cjDAE8-mHGHpA\\Mouse_Systemic\\Systemic-Injection_5E11\\MouseSystemic_5E11_neuroglancer_precomputed\\layers_split\n",
    "http-server . -p 8080 --cors\n",
    "```\n",
    "\n",
    "### Mouse Local (5E+11 vg/mL) 300 µL dose\n",
    "```\n",
    "cd /d D:\\BCH-cjDAE8-mHGHpA\\Mouse_Systemic\\Systemic-Injection_5E11\\MouseSystemic_5E11_neuroglancer_precomputed\\layers_split\n",
    "http-server . -p 8080 --cors\n",
    "```\n",
    "\n",
    "### Neuroglancer URL (DAPI, aGFP, aRFP, aTH)\n",
    "```\n",
    "https://neuroglancer-demo.appspot.com/#!{\n",
    "  \"layers\": {\n",
    "    \"DAPI\": {\n",
    "      \"type\": \"image\",\n",
    "      \"source\": \"precomputed://http://127.0.0.1:8080/DAPI/\",\n",
    "      \"shader\": \"#uicontrol invlerp normalized\\nvoid main(){ emitRGB(vec3(0.0, normalized(), normalized())); }\",\n",
    "      \"shaderControls\": { \"normalized\": { \"range\": [0, 65535] } },\n",
    "      \"blend\": \"additive\",\n",
    "      \"opacity\": 1.0\n",
    "    },\n",
    "    \"aGFP\": {\n",
    "      \"type\": \"image\",\n",
    "      \"source\": \"precomputed://http://127.0.0.1:8080/aGFP/\",\n",
    "      \"shader\": \"#uicontrol invlerp normalized\\nvoid main(){ emitRGB(vec3(0.0, normalized(), 0.0)); }\",\n",
    "      \"shaderControls\": { \"normalized\": { \"range\": [0, 65535] } },\n",
    "      \"blend\": \"additive\",\n",
    "      \"opacity\": 1.0\n",
    "    },\n",
    "    \"aRFP\": {\n",
    "      \"type\": \"image\",\n",
    "      \"source\": \"precomputed://http://127.0.0.1:8080/aRFP/\",\n",
    "      \"shader\": \"#uicontrol invlerp normalized\\nvoid main(){ emitRGB(vec3(normalized(), 0.0, 0.0)); }\",\n",
    "      \"shaderControls\": { \"normalized\": { \"range\": [0, 65535] } },\n",
    "      \"blend\": \"additive\",\n",
    "      \"opacity\": 1.0\n",
    "    },\n",
    "    \"aTH\": {\n",
    "      \"type\": \"image\",\n",
    "      \"source\": \"precomputed://http://127.0.0.1:8080/aTH/\",\n",
    "      \"shader\": \"#uicontrol invlerp normalized\\nvoid main(){ emitRGB(vec3(0.0, 0.0, normalized())); }\",\n",
    "      \"shaderControls\": { \"normalized\": { \"range\": [0, 65535] } },\n",
    "      \"blend\": \"additive\",\n",
    "      \"opacity\": 1.0\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "### Neuroglancer URL (DAPI, EGFP, dTom, aTH)\n",
    "```\n",
    "https://neuroglancer-demo.appspot.com/#!{\n",
    "  \"layers\": {\n",
    "    \"DAPI\": {\n",
    "      \"type\": \"image\",\n",
    "      \"source\": \"precomputed://http://127.0.0.1:8080/DAPI/\",\n",
    "      \"shader\": \"#uicontrol invlerp normalized\\nvoid main(){ emitRGB(vec3(0.0, normalized(), normalized())); }\",\n",
    "      \"shaderControls\": { \"normalized\": { \"range\": [0, 65535] } },\n",
    "      \"blend\": \"additive\",\n",
    "      \"opacity\": 1.0\n",
    "    },\n",
    "    \"EGFP\": {\n",
    "      \"type\": \"image\",\n",
    "      \"source\": \"precomputed://http://127.0.0.1:8080/EGFP/\",\n",
    "      \"shader\": \"#uicontrol invlerp normalized\\nvoid main(){ emitRGB(vec3(0.0, normalized(), 0.0)); }\",\n",
    "      \"shaderControls\": { \"normalized\": { \"range\": [0, 65535] } },\n",
    "      \"blend\": \"additive\",\n",
    "      \"opacity\": 1.0\n",
    "    },\n",
    "    \"dTom\": {\n",
    "      \"type\": \"image\",\n",
    "      \"source\": \"precomputed://http://127.0.0.1:8080/dTom/\",\n",
    "      \"shader\": \"#uicontrol invlerp normalized\\nvoid main(){ emitRGB(vec3(normalized(), 0.0, 0.0)); }\",\n",
    "      \"shaderControls\": { \"normalized\": { \"range\": [0, 65535] } },\n",
    "      \"blend\": \"additive\",\n",
    "      \"opacity\": 1.0\n",
    "    },\n",
    "    \"aTH\": {\n",
    "      \"type\": \"image\",\n",
    "      \"source\": \"precomputed://http://127.0.0.1:8080/aTH/\",\n",
    "      \"shader\": \"#uicontrol invlerp normalized\\nvoid main(){ emitRGB(vec3(0.0, 0.0, normalized())); }\",\n",
    "      \"shaderControls\": { \"normalized\": { \"range\": [0, 65535] } },\n",
    "      \"blend\": \"additive\",\n",
    "      \"opacity\": 1.0\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "### Neuroglancer URL (Nissl, aTH, dTom, aRFP)\n",
    "```\n",
    "https://neuroglancer-demo.appspot.com/#!{\n",
    "  \"layers\": {\n",
    "    \"Nissl\": {\n",
    "      \"type\": \"image\",\n",
    "      \"source\": \"precomputed://http://127.0.0.1:8080/Nissl/\",\n",
    "      \"shader\": \"#uicontrol invlerp normalized\\nvoid main(){ emitRGB(vec3(0.0, normalized(), normalized())); }\",\n",
    "      \"shaderControls\": { \"normalized\": { \"range\": [0, 65535] } },\n",
    "      \"blend\": \"additive\",\n",
    "      \"opacity\": 1.0\n",
    "    },\n",
    "    \"aTH\": {\n",
    "      \"type\": \"image\",\n",
    "      \"source\": \"precomputed://http://127.0.0.1:8080/aTH/\",\n",
    "      \"shader\": \"#uicontrol invlerp normalized\\nvoid main(){ emitRGB(vec3(0.0, normalized(), 0.0)); }\",\n",
    "      \"shaderControls\": { \"normalized\": { \"range\": [0, 65535] } },\n",
    "      \"blend\": \"additive\",\n",
    "      \"opacity\": 1.0\n",
    "    },\n",
    "    \"dTom\": {\n",
    "      \"type\": \"image\",\n",
    "      \"source\": \"precomputed://http://127.0.0.1:8080/dTom/\",\n",
    "      \"shader\": \"#uicontrol invlerp normalized\\nvoid main(){ emitRGB(vec3(normalized(), 0.0, 0.0)); }\",\n",
    "      \"shaderControls\": { \"normalized\": { \"range\": [0, 65535] } },\n",
    "      \"blend\": \"additive\",\n",
    "      \"opacity\": 1.0\n",
    "    },\n",
    "    \"aRFP\": {\n",
    "      \"type\": \"image\",\n",
    "      \"source\": \"precomputed://http://127.0.0.1:8080/aRFP/\",\n",
    "      \"shader\": \"#uicontrol invlerp normalized\\nvoid main(){ float v = normalized(); emitRGB(vec3(v, 0.0, v)); }\",\n",
    "      \"shaderControls\": { \"normalized\": { \"range\": [0, 65535] } },\n",
    "      \"blend\": \"additive\",\n",
    "      \"opacity\": 1.0\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "### Neuroglancer URL (DAPI, aTH, dTom, aRFP)\n",
    "```\n",
    "https://neuroglancer-demo.appspot.com/#!{\n",
    "  \"layers\": {\n",
    "    \"DAPI\": {\n",
    "      \"type\": \"image\",\n",
    "      \"source\": \"precomputed://http://127.0.0.1:8080/DAPI/\",\n",
    "      \"shader\": \"#uicontrol invlerp normalized\\nvoid main(){ emitRGB(vec3(0.0, normalized(), normalized())); }\",\n",
    "      \"shaderControls\": { \"normalized\": { \"range\": [0, 65535] } },\n",
    "      \"blend\": \"additive\",\n",
    "      \"opacity\": 1.0\n",
    "    },\n",
    "    \"aTH\": {\n",
    "      \"type\": \"image\",\n",
    "      \"source\": \"precomputed://http://127.0.0.1:8080/aTH/\",\n",
    "      \"shader\": \"#uicontrol invlerp normalized\\nvoid main(){ emitRGB(vec3(0.0, normalized(), 0.0)); }\",\n",
    "      \"shaderControls\": { \"normalized\": { \"range\": [0, 65535] } },\n",
    "      \"blend\": \"additive\",\n",
    "      \"opacity\": 1.0\n",
    "    },\n",
    "    \"dTom\": {\n",
    "      \"type\": \"image\",\n",
    "      \"source\": \"precomputed://http://127.0.0.1:8080/dTom/\",\n",
    "      \"shader\": \"#uicontrol invlerp normalized\\nvoid main(){ emitRGB(vec3(normalized(), 0.0, 0.0)); }\",\n",
    "      \"shaderControls\": { \"normalized\": { \"range\": [0, 65535] } },\n",
    "      \"blend\": \"additive\",\n",
    "      \"opacity\": 1.0\n",
    "    },\n",
    "    \"aRFP\": {\n",
    "      \"type\": \"image\",\n",
    "      \"source\": \"precomputed://http://127.0.0.1:8080/aRFP/\",\n",
    "      \"shader\": \"#uicontrol invlerp normalized\\nvoid main(){ float v = normalized(); emitRGB(vec3(v, 0.0, v)); }\",\n",
    "      \"shaderControls\": { \"normalized\": { \"range\": [0, 65535] } },\n",
    "      \"blend\": \"additive\",\n",
    "      \"opacity\": 1.0\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bc1c21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
